#!/usr/bin/env python3
"""
Comprehensive test runner for the compute platform
"""
import sys
import os
import subprocess
from datetime import datetime

def run_test_file(test_file, description):
    """Run a test file and return results"""
    print(f"\n{'='*60}")
    print(f"Running {description}")
    print(f"File: {test_file}")
    print(f"{'='*60}")
    
    try:
        result = subprocess.run([sys.executable, test_file], 
                              capture_output=True, text=True, cwd=os.path.dirname(__file__))
        
        print(result.stdout)
        if result.stderr:
            print("STDERR:", result.stderr)
        
        success = result.returncode == 0
        print(f"\n{'âœ… PASSED' if success else 'âŒ FAILED'}: {description}")
        return success
        
    except Exception as e:
        print(f"âŒ ERROR running {test_file}: {e}")
        return False


def main():
    """Run all tests"""
    print("ğŸš€ Compute Platform - Comprehensive Test Suite")
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # Define test files and descriptions
    tests = [
        ("test_models.py", "Data Models Validation"),
        ("test_simple_api.py", "Basic API Functionality"),
        ("test_comprehensive.py", "Comprehensive Model Testing"),
        ("test_integration.py", "Integration & Real-World Scenarios")
    ]
    
    results = []
    total_tests = len(tests)
    
    for test_file, description in tests:
        if os.path.exists(test_file):
            success = run_test_file(test_file, description)
            results.append((description, success))
        else:
            print(f"âš ï¸ Test file not found: {test_file}")
            results.append((description, False))
    
    # Summary
    print("\n" + "="*80)
    print("ğŸ“Š TEST SUMMARY")
    print("="*80)
    
    passed = sum(1 for _, success in results if success)
    
    for description, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        print(f"{status}: {description}")
    
    print(f"\nğŸ“ˆ Overall Results: {passed}/{total_tests} test suites passed")
    
    if passed == total_tests:
        print("ğŸ‰ ALL TESTS PASSED! The compute platform is working correctly.")
        print("\nğŸ”§ Platform Status:")
        print("   âœ… Data models are valid and functional")
        print("   âœ… Core services can be instantiated")
        print("   âœ… JSON serialization works properly")
        print("   âœ… Model validation is working")
        print("   âœ… Complex workflows can be created")
        print("   âœ… Multi-user scenarios work")
        print("   âœ… Resource management is functional")
        print("   âœ… Pipeline orchestration works")
        
        print("\nğŸš€ Ready for:")
        print("   â€¢ ML experiment tracking")
        print("   â€¢ Model deployment and serving")
        print("   â€¢ Data pipeline orchestration")
        print("   â€¢ Multi-user collaboration")
        print("   â€¢ Resource allocation and management")
        
        print("\nâš ï¸ Note: This is a testing environment.")
        print("   For production use, implement:")
        print("   â€¢ Database persistence")
        print("   â€¢ Authentication/authorization")
        print("   â€¢ API security")
        print("   â€¢ Monitoring and logging")
        print("   â€¢ Error handling and recovery")
        
    else:
        print(f"âš ï¸ {total_tests - passed} test suite(s) failed. Review the output above.")
        print("\nğŸ” Common issues to check:")
        print("   â€¢ Missing dependencies")
        print("   â€¢ Import path issues")
        print("   â€¢ Model validation errors")
        print("   â€¢ Service configuration problems")
    
    print(f"\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return passed == total_tests


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 